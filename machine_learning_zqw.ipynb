{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation, no need to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Setup the Environment\n",
    "\n",
    "# !pip install numpy pandas tensorflow keras opencv-python scikit-learn matplotlib\n",
    "# !pip install torch torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 129868\n",
      "Number of validation samples: 11335\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Paths\n",
    "train_annotations_path = './train_coco_annotations.csv'\n",
    "train_folder = './data/train/'\n",
    "valid_annotations_path = './valid_coco_annotations.csv'\n",
    "valid_folder = './data/valid/'\n",
    "\n",
    "# Function to load data\n",
    "def load_data(annotations_path, folder):\n",
    "    annotations = pd.read_csv(annotations_path)\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "\n",
    "    for _, row in annotations.iterrows():\n",
    "        image_id = row['FileName']\n",
    "        category = row['Category']  # 0 for holds, 1 for volumes\n",
    "        image_path = os.path.join(folder, f\"{image_id}\")\n",
    "\n",
    "        if os.path.exists(image_path):\n",
    "            image_paths.append(image_path)\n",
    "            labels.append(category)\n",
    "        else:\n",
    "            print(f\"Image not found: {image_path}\")\n",
    "\n",
    "    return image_paths, labels\n",
    "\n",
    "# Load training data\n",
    "train_image_paths, train_labels = load_data(train_annotations_path, train_folder)\n",
    "\n",
    "# Load validation data\n",
    "valid_image_paths, valid_labels = load_data(valid_annotations_path, valid_folder)\n",
    "\n",
    "# Convert labels to numeric\n",
    "unique_classes = sorted(set(train_labels + valid_labels))\n",
    "class_to_idx = {cls: idx for idx, cls in enumerate(unique_classes)}\n",
    "train_labels_numeric = [class_to_idx[label] for label in train_labels]\n",
    "valid_labels_numeric = [class_to_idx[label] for label in valid_labels]\n",
    "\n",
    "# Print summary\n",
    "print(f\"Number of training samples: {len(train_image_paths)}\")\n",
    "print(f\"Number of validation samples: {len(valid_image_paths)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # debugging codes\n",
    "# print(f\"Labels: {labels}\")\n",
    "# print(f\"Unique Classes: {unique_classes}\")\n",
    "# print(f\"Class to Index Mapping: {class_to_idx}\")\n",
    "# print(f\"Labels Numeric: {labels_numeric}\")\n",
    "\n",
    "# for path in image_paths:\n",
    "#     if not os.path.exists(path):\n",
    "#         print(f\"Missing file: {path}\")\n",
    "\n",
    "# print(f\"Train Folder: {train_folder}\")\n",
    "# print(os.listdir(train_folder))\n",
    "\n",
    "# print(f\"Annotations Dataset Shape: {annotations.shape}\")\n",
    "# print(annotations.head())\n",
    "# print(f\"Columns in Dataset: {annotations.columns}\")\n",
    "# print(annotations['Category'].unique())  # Replace 'label_column' with the actual column name\n",
    "\n",
    "# labels = annotations['Category'].tolist()  # Replace 'label_column' with the actual column name\n",
    "# print(f\"Extracted Labels: {labels}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training] Progress: 1.75% (2267/129868)\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 69\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] Processing completed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# Process training data\u001b[39;00m\n\u001b[1;32m---> 69\u001b[0m \u001b[43mprocess_annotations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_annotations_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_images_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTraining\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# Process validation data\u001b[39;00m\n\u001b[0;32m     72\u001b[0m process_annotations(valid_annotations_path, valid_folder, valid_images_folder, valid_labels_folder, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[16], line 43\u001b[0m, in \u001b[0;36mprocess_annotations\u001b[1;34m(annotations_path, src_folder, dest_images_folder, dest_labels_folder, dataset_name)\u001b[0m\n\u001b[0;32m     40\u001b[0m shutil\u001b[38;5;241m.\u001b[39mcopy(src_image_path, dest_image_path)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Load image to get dimensions\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_image_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m image \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     45\u001b[0m     img_height, img_width \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m]\n",
      "File \u001b[1;32md:\\zqw\\2024fall\\SI670\\.venv\\Lib\\site-packages\\ultralytics\\utils\\patches.py:26\u001b[0m, in \u001b[0;36mimread\u001b[1;34m(filename, flags)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mimread\u001b[39m(filename: \u001b[38;5;28mstr\u001b[39m, flags: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mIMREAD_COLOR):\n\u001b[0;32m     16\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03m    Read an image from a file.\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;124;03m        (np.ndarray): The read image.\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muint8\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import cv2\n",
    "\n",
    "# Paths\n",
    "train_annotations_path = './train_coco_annotations.csv'\n",
    "valid_annotations_path = './valid_coco_annotations.csv'\n",
    "train_folder = './data/train/'\n",
    "valid_folder = './data/valid/'\n",
    "yolo_data_folder = './yolo_data/'\n",
    "\n",
    "# Create new YOLO data folders\n",
    "train_images_folder = os.path.join(yolo_data_folder, 'train/images/')\n",
    "train_labels_folder = os.path.join(yolo_data_folder, 'train/labels/')\n",
    "valid_images_folder = os.path.join(yolo_data_folder, 'valid/images/')\n",
    "valid_labels_folder = os.path.join(yolo_data_folder, 'valid/labels/')\n",
    "\n",
    "os.makedirs(train_images_folder, exist_ok=True)\n",
    "os.makedirs(train_labels_folder, exist_ok=True)\n",
    "os.makedirs(valid_images_folder, exist_ok=True)\n",
    "os.makedirs(valid_labels_folder, exist_ok=True)\n",
    "\n",
    "# Function to copy and convert COCO annotations to YOLO format\n",
    "def process_annotations(annotations_path, src_folder, dest_images_folder, dest_labels_folder, dataset_name):\n",
    "    annotations = pd.read_csv(annotations_path)\n",
    "    total_annotations = len(annotations)\n",
    "\n",
    "    for idx, row in enumerate(annotations.iterrows(), start=1):\n",
    "        _, row_data = row\n",
    "        image_id = row_data['FileName']\n",
    "        bbox = eval(row_data['BBox'])  # Convert BBox string to list\n",
    "        class_id = row_data['Category']  # 0 for holds, 1 for volumes\n",
    "        src_image_path = os.path.join(src_folder, image_id)\n",
    "        dest_image_path = os.path.join(dest_images_folder, image_id)\n",
    "        yolo_label_path = os.path.join(dest_labels_folder, os.path.splitext(image_id)[0] + \".txt\")\n",
    "\n",
    "        # Copy image\n",
    "        if os.path.exists(src_image_path):\n",
    "            shutil.copy(src_image_path, dest_image_path)\n",
    "\n",
    "            # Load image to get dimensions\n",
    "            image = cv2.imread(src_image_path)\n",
    "            if image is not None:\n",
    "                img_height, img_width = image.shape[:2]\n",
    "\n",
    "                # Convert bbox to YOLO format\n",
    "                x, y, w, h = bbox\n",
    "                x_center = (x + w / 2) / img_width\n",
    "                y_center = (y + h / 2) / img_height\n",
    "                width = w / img_width\n",
    "                height = h / img_height\n",
    "\n",
    "                # Write YOLO annotation\n",
    "                with open(yolo_label_path, \"a\") as yolo_file:\n",
    "                    yolo_file.write(f\"{class_id} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\\n\")\n",
    "            else:\n",
    "                print(f\"Could not load image: {src_image_path}\")\n",
    "        else:\n",
    "            print(f\"Image not found: {src_image_path}\")\n",
    "\n",
    "        # Calculate and display progress\n",
    "        progress = (idx / total_annotations) * 100\n",
    "        print(f\"[{dataset_name}] Progress: {progress:.2f}% ({idx}/{total_annotations})\", end=\"\\r\")\n",
    "\n",
    "    print(f\"\\n[{dataset_name}] Processing completed.\")\n",
    "\n",
    "# Process training data\n",
    "process_annotations(train_annotations_path, train_folder, train_images_folder, train_labels_folder, \"Training\")\n",
    "\n",
    "# Process validation data\n",
    "process_annotations(valid_annotations_path, valid_folder, valid_images_folder, valid_labels_folder, \"Validation\")\n",
    "\n",
    "print(\"Dataset organized and annotations converted to YOLO format.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run YOLO in yolov5 folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Code for running YOLO  \n",
    "python train.py --img 640 --batch 16 --epochs 50 --data data.yaml --weights yolov5s.pt --project runs --name hold_volume_detection\n",
    "\n",
    "- current:   \n",
    "python train.py --img 416 --batch 8 --epochs 10 --data data.yaml --weights yolov5n.pt --project runs --name hold_volume_detection --workers 4\n",
    "Results saved to runs\\hold_volume_detection5\n",
    "\n",
    "- to resume a run  \n",
    "python train.py --resume\n",
    "\n",
    "- can try this  \n",
    "python train.py --img 416 --batch 8 --epochs 10 --data data.yaml --weights yolov5n.pt --hyp hyp.scratch-low.yaml --workers 4\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Results saved to runs\\hold_volume_detection5\n",
    "\n",
    "10 epochs completed in 0.818 hours.\n",
    "Optimizer stripped from runs\\hold_volume_detection5\\weights\\last.pt, 3.7MB\n",
    "Optimizer stripped from runs\\hold_volume_detection5\\weights\\best.pt, 3.7MB\n",
    "\n",
    "Validating runs\\hold_volume_detection5\\weights\\best.pt...\n",
    "Fusing layers... \n",
    "Model summary: 157 layers, 1761871 parameters, 0 gradients, 4.1 GFLOPs\n",
    "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 18/18 [00:09<00:00,  1.93it/s]\n",
    "                   all        277      11335      0.782      0.616      0.683      0.409\n",
    "                  hold        277      10421      0.755      0.638      0.665       0.38\n",
    "                volume        277        914      0.809      0.595        0.7      0.439\n",
    "Results saved to runs\\hold_volume_detection5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# Determine the current working directory\n",
    "script_dir = os.getcwd()\n",
    "\n",
    "\n",
    "# Change directory to the YOLOv5 folder\n",
    "yolo_folder = os.path.join(script_dir, \"yolov5\")\n",
    "os.chdir(yolo_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changed working directory to: d:\\zqw\\2024fall\\SI670\\si670_final_project\\yolov5\n",
      "Setup complete!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Verify the working directory\n",
    "print(f\"Changed working directory to: {os.getcwd()}\")\n",
    "\n",
    "# Add YOLOv5 folder to Python path\n",
    "yolov5_path = os.path.join(script_dir, \"yolov5\")\n",
    "sys.path.append(yolov5_path)\n",
    "\n",
    "# Import YOLOv5 components\n",
    "from models.common import DetectMultiBackend\n",
    "\n",
    "# Set paths\n",
    "model_path = os.path.join(script_dir, \"runs/hold_volume_detection5/weights/best.pt\")\n",
    "input_folder = os.path.join(script_dir, \"data/test_image\")\n",
    "output_folder = os.path.join(script_dir, \"data/test_output\")\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy.strings'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Load the YOLOv5 model\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mDetectMultiBackend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel loaded successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Initialize statistics\u001b[39;00m\n",
      "File \u001b[1;32md:\\zqw\\2024fall\\SI670\\si670_final_project\\yolov5\\models\\common.py:480\u001b[0m, in \u001b[0;36mDetectMultiBackend.__init__\u001b[1;34m(self, weights, device, dnn, data, fp16, fuse)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m    479\u001b[0m w \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(weights[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(weights, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m weights)\n\u001b[1;32m--> 480\u001b[0m pt, jit, onnx, xml, engine, coreml, saved_model, pb, tflite, edgetpu, tfjs, paddle, triton \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    481\u001b[0m fp16 \u001b[38;5;241m&\u001b[39m\u001b[38;5;241m=\u001b[39m pt \u001b[38;5;129;01mor\u001b[39;00m jit \u001b[38;5;129;01mor\u001b[39;00m onnx \u001b[38;5;129;01mor\u001b[39;00m engine \u001b[38;5;129;01mor\u001b[39;00m triton  \u001b[38;5;66;03m# FP16\u001b[39;00m\n\u001b[0;32m    482\u001b[0m nhwc \u001b[38;5;241m=\u001b[39m coreml \u001b[38;5;129;01mor\u001b[39;00m saved_model \u001b[38;5;129;01mor\u001b[39;00m pb \u001b[38;5;129;01mor\u001b[39;00m tflite \u001b[38;5;129;01mor\u001b[39;00m edgetpu  \u001b[38;5;66;03m# BHWC formats (vs torch BCWH)\u001b[39;00m\n",
      "File \u001b[1;32md:\\zqw\\2024fall\\SI670\\si670_final_project\\yolov5\\models\\common.py:781\u001b[0m, in \u001b[0;36mDetectMultiBackend._model_type\u001b[1;34m(p)\u001b[0m\n\u001b[0;32m    775\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    776\u001b[0m \u001b[38;5;124;03mDetermines model type from file path or URL, supporting various export formats.\u001b[39;00m\n\u001b[0;32m    777\u001b[0m \n\u001b[0;32m    778\u001b[0m \u001b[38;5;124;03mExample: path='path/to/model.onnx' -> type=onnx\u001b[39;00m\n\u001b[0;32m    779\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    780\u001b[0m \u001b[38;5;66;03m# types = [pt, jit, onnx, xml, engine, coreml, saved_model, pb, tflite, edgetpu, tfjs, paddle]\u001b[39;00m\n\u001b[1;32m--> 781\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexport\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m export_formats\n\u001b[0;32m    782\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdownloads\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_url\n\u001b[0;32m    784\u001b[0m sf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(export_formats()\u001b[38;5;241m.\u001b[39mSuffix)  \u001b[38;5;66;03m# export suffixes\u001b[39;00m\n",
      "File \u001b[1;32md:\\zqw\\2024fall\\SI670\\si670_final_project\\yolov5\\export.py:71\u001b[0m\n\u001b[0;32m     68\u001b[0m     ROOT \u001b[38;5;241m=\u001b[39m Path(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mrelpath(ROOT, Path\u001b[38;5;241m.\u001b[39mcwd()))  \u001b[38;5;66;03m# relative\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m attempt_load\n\u001b[1;32m---> 71\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01myolo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ClassificationModel, Detect, DetectionModel, SegmentationModel\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataloaders\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LoadImages\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneral\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     74\u001b[0m     LOGGER,\n\u001b[0;32m     75\u001b[0m     Profile,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     86\u001b[0m     yaml_save,\n\u001b[0;32m     87\u001b[0m )\n",
      "File \u001b[1;32md:\\zqw\\2024fall\\SI670\\si670_final_project\\yolov5\\models\\yolo.py:55\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautoanchor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_anchor_order\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneral\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LOGGER, check_version, check_yaml, colorstr, make_divisible, print_args\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplots\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m feature_visualization\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtorch_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     57\u001b[0m     fuse_conv_and_bn,\n\u001b[0;32m     58\u001b[0m     initialize_weights,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     63\u001b[0m     time_sync,\n\u001b[0;32m     64\u001b[0m )\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32md:\\zqw\\2024fall\\SI670\\si670_final_project\\yolov5\\utils\\plots.py:18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image, ImageDraw\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mndimage\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfilters\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gaussian_filter1d\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01multralytics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplotting\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Annotator\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TryExcept, threaded\n",
      "File \u001b[1;32md:\\zqw\\2024fall\\SI670\\.venv\\Lib\\site-packages\\scipy\\ndimage\\__init__.py:152\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03m=========================================================\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mMultidimensional image processing (:mod:`scipy.ndimage`)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    119\u001b[0m \n\u001b[0;32m    120\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# Copyright (C) 2003-2005 Peter J. Verveer\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# Redistribution and use in source and binary forms, with or without\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;66;03m# NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\u001b[39;00m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;66;03m# SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\u001b[39;00m\n\u001b[1;32m--> 152\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_filters\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_fourier\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_interpolation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32md:\\zqw\\2024fall\\SI670\\.venv\\Lib\\site-packages\\scipy\\ndimage\\_filters.py:37\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01moperator\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_util\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m normalize_axis_index\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _ni_support\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _nd_image\n",
      "File \u001b[1;32md:\\zqw\\2024fall\\SI670\\.venv\\Lib\\site-packages\\scipy\\_lib\\_util.py:18\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     11\u001b[0m     Optional,\n\u001b[0;32m     12\u001b[0m     Union,\n\u001b[0;32m     13\u001b[0m     TYPE_CHECKING,\n\u001b[0;32m     14\u001b[0m     TypeVar,\n\u001b[0;32m     15\u001b[0m )\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_array_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m array_namespace, is_numpy, size \u001b[38;5;28;01mas\u001b[39;00m xp_size\n\u001b[0;32m     21\u001b[0m AxisError: \u001b[38;5;28mtype\u001b[39m[\u001b[38;5;167;01mException\u001b[39;00m]\n\u001b[0;32m     22\u001b[0m ComplexWarning: \u001b[38;5;28mtype\u001b[39m[\u001b[38;5;167;01mWarning\u001b[39;00m]\n",
      "File \u001b[1;32md:\\zqw\\2024fall\\SI670\\.venv\\Lib\\site-packages\\scipy\\_lib\\_array_api.py:21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnpt\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m array_api_compat\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marray_api_compat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     22\u001b[0m     is_array_api_obj,\n\u001b[0;32m     23\u001b[0m     size,\n\u001b[0;32m     24\u001b[0m     numpy \u001b[38;5;28;01mas\u001b[39;00m np_compat,\n\u001b[0;32m     25\u001b[0m     device\n\u001b[0;32m     26\u001b[0m )\n\u001b[0;32m     28\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marray_namespace\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_asarray\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# To enable array API and strict array-like input validation\u001b[39;00m\n",
      "File \u001b[1;32md:\\zqw\\2024fall\\SI670\\.venv\\Lib\\site-packages\\scipy\\_lib\\array_api_compat\\numpy\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# from numpy import * doesn't overwrite these builtin names\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;28mabs\u001b[39m, \u001b[38;5;28mmax\u001b[39m, \u001b[38;5;28mmin\u001b[39m, \u001b[38;5;28mround\u001b[39m \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[1;32md:\\zqw\\2024fall\\SI670\\.venv\\Lib\\site-packages\\numpy\\__init__.py:390\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(attr)\u001b[0m\n\u001b[0;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy.strings'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load the YOLOv5 model\n",
    "model = DetectMultiBackend(model_path, device=torch.device('cpu'))\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# Initialize statistics\n",
    "stats = {\"class\": [], \"confidence\": [], \"image\": [], \"x1\": [], \"y1\": [], \"x2\": [], \"y2\": [], \"cls\": []}\n",
    "\n",
    "# Loop through all images in the input folder\n",
    "image_files = [f for f in os.listdir(input_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "for image_name in tqdm(image_files, desc=\"Processing images\", unit=\"image\"):\n",
    "    try:\n",
    "        image_path = os.path.join(input_folder, image_name)\n",
    "\n",
    "        # Load image\n",
    "        img = cv2.imread(image_path)\n",
    "        if img is None:\n",
    "            print(f\"Warning: Unable to read image {image_name}. Skipping.\")\n",
    "            continue\n",
    "        img_height, img_width = img.shape[:2]\n",
    "\n",
    "        # Preprocess image for YOLO\n",
    "        img_resized = cv2.resize(img, (416, 416))  # Resize to YOLO input size\n",
    "        img_tensor = torch.from_numpy(img_resized).permute(2, 0, 1).float().div(255.0).unsqueeze(0)  # Normalize and add batch dimension\n",
    "\n",
    "        # Run inference\n",
    "        results = model(img_tensor)\n",
    "\n",
    "        # Process predictions\n",
    "        for det in results[0]:  # Get the first level of predictions\n",
    "            # Ensure det is a tensor and process it\n",
    "            if isinstance(det, torch.Tensor):\n",
    "                det = det.cpu().numpy()  # Convert tensor to numpy array\n",
    "\n",
    "            for pred in det:  # Iterate through individual predictions\n",
    "                # Extract relevant values: bbox, confidence, and class\n",
    "                if pred.shape[0] >= 6:  # Ensure there are enough elements\n",
    "                    x_center, y_center, width, height = pred[:4]\n",
    "                    conf = pred[4]  # Fifth value is confidence\n",
    "                    cls = pred[5]  # Sixth value is class index\n",
    "\n",
    "                    \n",
    "\n",
    "                    # Confidence threshold\n",
    "                    if conf < 0.5:  # Ignore predictions with low confidence\n",
    "                        print(f\"Skipped prediction with low confidence: {conf}\")\n",
    "                        continue\n",
    "\n",
    "                    # Convert from center-size format to corner coordinates\n",
    "                    x1 = int((x_center - width / 2) / 416 * img_width)\n",
    "                    y1 = int((y_center - height / 2) / 416 * img_height)\n",
    "                    x2 = int((x_center + width / 2) / 416 * img_width)\n",
    "                    y2 = int((y_center + height / 2) / 416 * img_height)\n",
    "\n",
    "                    class_id = 0 if cls <= 0.5 else 1  # 0 for holds, 1 for volumes\n",
    "\n",
    "                    # Save bounding box locations\n",
    "                    stats[\"class\"].append(int(class_id))\n",
    "                    stats[\"confidence\"].append(float(conf))\n",
    "                    stats[\"image\"].append(image_name)\n",
    "                    stats[\"x1\"].append(x1)\n",
    "                    stats[\"y1\"].append(y1)\n",
    "                    stats[\"x2\"].append(x2)\n",
    "                    stats[\"y2\"].append(y2)\n",
    "                    stats[\"cls\"].append(cls)\n",
    "\n",
    "                    label = f\"{'Hold' if cls <= 0.5 else 'Volume'} {conf:.2f}\"\n",
    "                    color = (255, 0, 0) if label=='Hold' else (0, 0, 255)  # Blue for holds, red for volumes\n",
    "                    cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)\n",
    "                    # Draw bounding boxes\n",
    "                    cv2.putText(img, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "        # Save the annotated image\n",
    "        output_path = os.path.join(output_folder, image_name)\n",
    "        cv2.imwrite(output_path, img)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_name}: {e}\")\n",
    "\n",
    "# Save statistics to a CSV file\n",
    "stats_df = pd.DataFrame(stats)\n",
    "stats_df.to_csv(os.path.join(output_folder, \"test_statistics.csv\"), index=False)\n",
    "\n",
    "print(f\"Processed {len(image_files)} images. Results saved to {output_folder}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMS applied. Results saved to d:\\zqw\\2024fall\\SI670\\si670_final_project\\yolov5\\data/test_output\\nms_statistics.csv.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def iou(box1, box2):\n",
    "    \"\"\"\n",
    "    Calculate Intersection over Union (IoU) between two bounding boxes.\n",
    "    box1, box2: (x1, y1, x2, y2)\n",
    "    \"\"\"\n",
    "    x1_inter = max(box1[0], box2[0])\n",
    "    y1_inter = max(box1[1], box2[1])\n",
    "    x2_inter = min(box1[2], box2[2])\n",
    "    y2_inter = min(box1[3], box2[3])\n",
    "\n",
    "    inter_area = max(0, x2_inter - x1_inter + 1) * max(0, y2_inter - y1_inter + 1)\n",
    "    box1_area = (box1[2] - box1[0] + 1) * (box1[3] - box1[1] + 1)\n",
    "    box2_area = (box2[2] - box2[0] + 1) * (box2[3] - box2[1] + 1)\n",
    "    union_area = box1_area + box2_area - inter_area\n",
    "\n",
    "    return inter_area / union_area if union_area != 0 else 0\n",
    "\n",
    "def non_maximum_suppression(df, iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Apply Non-Maximum Suppression (NMS) to the bounding boxes in a DataFrame.\n",
    "    df: DataFrame containing 'x1', 'y1', 'x2', 'y2', 'confidence', 'class', 'image'\n",
    "    iou_threshold: IoU threshold to suppress overlapping boxes\n",
    "    \"\"\"\n",
    "    nms_results = []\n",
    "\n",
    "    # Process each image separately\n",
    "    for image, group in df.groupby('image'):\n",
    "        boxes = group[['x1', 'y1', 'x2', 'y2']].values\n",
    "        confidences = group['confidence'].values\n",
    "        classes = group['class'].values\n",
    "\n",
    "        # Sort boxes by confidence score in descending order\n",
    "        indices = confidences.argsort()[::-1]\n",
    "        boxes = boxes[indices]\n",
    "        confidences = confidences[indices]\n",
    "        classes = classes[indices]\n",
    "        group = group.iloc[indices]  # Sort the group DataFrame accordingly\n",
    "\n",
    "        suppressed = set()\n",
    "        for i in range(len(boxes)):\n",
    "            if i in suppressed:\n",
    "                continue\n",
    "            current_box = boxes[i]\n",
    "            nms_results.append(group.iloc[i])  # Keep the box with the highest confidence\n",
    "            for j in range(i + 1, len(boxes)):\n",
    "                if j in suppressed:\n",
    "                    continue\n",
    "                iou_score = iou(current_box, boxes[j])\n",
    "                # Suppress boxes based on IoU only, ignoring class differences\n",
    "                if iou_score > iou_threshold:\n",
    "                    suppressed.add(j)\n",
    "\n",
    "    # Convert the results back to a DataFrame\n",
    "    return pd.DataFrame(nms_results)\n",
    "\n",
    "# Load the statistics DataFrame\n",
    "stats_path = os.path.join(output_folder, \"test_statistics.csv\")\n",
    "stats_df = pd.read_csv(stats_path)\n",
    "\n",
    "# Apply NMS\n",
    "nms_df = non_maximum_suppression(stats_df, iou_threshold=0.5)\n",
    "\n",
    "# Save the NMS results to a new CSV file\n",
    "nms_path = os.path.join(output_folder, \"nms_statistics.csv\")\n",
    "nms_df.to_csv(nms_path, index=False)\n",
    "\n",
    "print(f\"NMS applied. Results saved to {nms_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output folder for NMS results\n",
    "nms_output_folder = os.path.join(output_folder, \"nms_output\")\n",
    "os.makedirs(nms_output_folder, exist_ok=True)\n",
    "\n",
    "# Load NMS results\n",
    "nms_df = pd.read_csv(os.path.join(output_folder, \"nms_statistics.csv\"))\n",
    "\n",
    "# Loop through each image and draw bounding boxes\n",
    "for image_name, group in nms_df.groupby('image'):\n",
    "    image_path = os.path.join(input_folder, image_name)\n",
    "    output_path = os.path.join(nms_output_folder, image_name)\n",
    "\n",
    "    # Load image\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        print(f\"Warning: Unable to read image {image_name}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Draw bounding boxes from NMS\n",
    "    for _, row in group.iterrows():\n",
    "        x1, y1, x2, y2 = int(row['x1']), int(row['y1']), int(row['x2']), int(row['y2'])\n",
    "        cls = float(row['cls'])\n",
    "        conf = float(row['confidence'])\n",
    "\n",
    "        # Define color: blue for holds (class 0), red for volumes (class 1)\n",
    "        color = (255, 0, 0) if cls >= 0.5 else (0, 0, 255)\n",
    "        label = f\"{'Hold' if cls >= 0.5 else 'Volume'} {conf:.2f}\"\n",
    "\n",
    "        # Draw rectangle and label\n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)\n",
    "        cv2.putText(img, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "    # Save the annotated image\n",
    "    cv2.imwrite(output_path, img)\n",
    "\n",
    "print(f\"NMS drawings saved to: {nms_output_folder}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
