{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation, no need to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Setup the Environment\n",
    "\n",
    "!pip install numpy pandas tensorflow keras opencv-python scikit-learn matplotlib\n",
    "!pip install torch torchvision\n",
    "!pip install pandas\n",
    "!pip install -r ./yolov5/requirements.txt\n",
    "!pip install tqdm\n",
    "!pip install imgaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import shutil\n",
    "import sys\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import albumentations as A\n",
    "import imgaug.augmenters as iaa\n",
    "from imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "train_annotations_path = './train_coco_annotations.csv'\n",
    "valid_annotations_path = './valid_coco_annotations.csv'\n",
    "train_folder = './data/train/'\n",
    "valid_folder = './data/valid/'\n",
    "yolo_data_folder = './yolo_data/'\n",
    "\n",
    "# Create new YOLO data folders\n",
    "train_images_folder = os.path.join(yolo_data_folder, 'train/images/')\n",
    "train_labels_folder = os.path.join(yolo_data_folder, 'train/labels/')\n",
    "valid_images_folder = os.path.join(yolo_data_folder, 'valid/images/')\n",
    "valid_labels_folder = os.path.join(yolo_data_folder, 'valid/labels/')\n",
    "\n",
    "os.makedirs(train_images_folder, exist_ok=True)\n",
    "os.makedirs(train_labels_folder, exist_ok=True)\n",
    "os.makedirs(valid_images_folder, exist_ok=True)\n",
    "os.makedirs(valid_labels_folder, exist_ok=True)\n",
    "\n",
    "# Filter and save volume bounding boxes\n",
    "volume_bboxes = []\n",
    "\n",
    "# Function to copy and convert COCO annotations to YOLO format\n",
    "def process_annotations(annotations_path, src_folder, dest_images_folder, dest_labels_folder, dataset_name):\n",
    "    annotations = pd.read_csv(annotations_path)\n",
    "    total_annotations = len(annotations)\n",
    "\n",
    "    for idx, row in enumerate(annotations.iterrows(), start=1):\n",
    "        _, row_data = row\n",
    "        image_id = row_data['FileName']\n",
    "        bbox = eval(row_data['BBox'])  # Convert BBox string to list\n",
    "        class_id = row_data['Category']  # 0 for holds, 1 for volumes\n",
    "\n",
    "        # Filter for volumes\n",
    "        if class_id == 1:\n",
    "            volume_bboxes.append((image_id, bbox))  # Store for augmentation\n",
    "\n",
    "        src_image_path = os.path.join(src_folder, image_id)\n",
    "        dest_image_path = os.path.join(dest_images_folder, image_id)\n",
    "        yolo_label_path = os.path.join(dest_labels_folder, os.path.splitext(image_id)[0] + \".txt\")\n",
    "\n",
    "        # Copy image\n",
    "        if os.path.exists(src_image_path):\n",
    "            shutil.copy(src_image_path, dest_image_path)\n",
    "\n",
    "            # Load image to get dimensions\n",
    "            image = cv2.imread(src_image_path)\n",
    "            if image is not None:\n",
    "                img_height, img_width = image.shape[:2]\n",
    "\n",
    "                # Convert bbox to YOLO format\n",
    "                x, y, w, h = bbox\n",
    "                x_center = (x + w / 2) / img_width\n",
    "                y_center = (y + h / 2) / img_height\n",
    "                width = w / img_width\n",
    "                height = h / img_height\n",
    "\n",
    "                # Write YOLO annotation\n",
    "                with open(yolo_label_path, \"a\") as yolo_file:\n",
    "                    yolo_file.write(f\"{class_id} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\\n\")\n",
    "            else:\n",
    "                print(f\"Could not load image: {src_image_path}\")\n",
    "        else:\n",
    "            print(f\"Image not found: {src_image_path}\")\n",
    "\n",
    "        # Calculate and display progress\n",
    "        progress = (idx / total_annotations) * 100\n",
    "        print(f\"[{dataset_name}] Progress: {progress:.2f}% ({idx}/{total_annotations})\", end=\"\\r\")\n",
    "\n",
    "    print(f\"\\n[{dataset_name}] Processing completed.\")\n",
    "\n",
    "# Process training data\n",
    "process_annotations(train_annotations_path, train_folder, train_images_folder, train_labels_folder, \"Training\")\n",
    "\n",
    "# Process validation data\n",
    "process_annotations(valid_annotations_path, valid_folder, valid_images_folder, valid_labels_folder, \"Validation\")\n",
    "\n",
    "print(\"Dataset organized and annotations converted to YOLO format.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "- Due to unbalanced dataset (fewer \"volumes\" comparing to holds), we here specifically augmented volumes images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define augmentations\n",
    "augmenters = iaa.Sequential([\n",
    "    iaa.Fliplr(0.5),  # Horizontal flip\n",
    "    iaa.Affine(scale=(0.8, 1.2), translate_percent=(-0.2, 0.2), rotate=(-15, 15)),  # Scale, translate, rotate\n",
    "    iaa.Multiply((0.8, 1.2)),  # Adjust brightness\n",
    "])\n",
    "\n",
    "# Output folders\n",
    "train_images_folder = './augmented_images'\n",
    "train_labels_folder = './augmented_labels'\n",
    "\n",
    "os.makedirs(train_images_folder, exist_ok=True)\n",
    "os.makedirs(train_labels_folder, exist_ok=True)\n",
    "\n",
    "# Process each image and its bounding boxes\n",
    "for image_id, bbox in tqdm(volume_bboxes, desc=\"Augmenting images\"):\n",
    "    # Load the image\n",
    "    src_image_path = os.path.join('./data/train', image_id)\n",
    "    image = cv2.imread(src_image_path)\n",
    "    if image is None:\n",
    "        continue\n",
    "\n",
    "    img_height, img_width = image.shape[:2]\n",
    "\n",
    "    # Convert bounding box from [x_min, y_min, w, h] to [x_min, y_min, x_max, y_max]\n",
    "    x_min, y_min, w, h = bbox\n",
    "    x_max = x_min + w\n",
    "    y_max = y_min + h\n",
    "\n",
    "    # Create BoundingBoxesOnImage object\n",
    "    bbs = BoundingBoxesOnImage([\n",
    "        BoundingBox(x1=x_min, y1=y_min, x2=x_max, y2=y_max)\n",
    "    ], shape=image.shape)\n",
    "\n",
    "    try:\n",
    "        # Apply augmentation\n",
    "        image_aug, bbs_aug = augmenters(image=image, bounding_boxes=bbs)\n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "    # Convert augmented bounding boxes back to YOLO format\n",
    "    augmented_bboxes = []\n",
    "    for bb in bbs_aug:\n",
    "        bb = bb.clip_out_of_image(image.shape)  # Pass image.shape to clip bounding box\n",
    "        if bb.is_out_of_image(image.shape):  # Skip completely invalid bounding boxes\n",
    "            continue\n",
    "        x_min, y_min, x_max, y_max = bb.x1, bb.y1, bb.x2, bb.y2\n",
    "        w = x_max - x_min\n",
    "        h = y_max - y_min\n",
    "        augmented_bboxes.append([x_min, y_min, w, h])\n",
    "\n",
    "    # Skip if no valid bounding boxes remain\n",
    "    if not augmented_bboxes:\n",
    "        continue\n",
    "\n",
    "    # Save the augmented image\n",
    "    aug_image_id = f\"aug_{os.path.basename(image_id)}\"\n",
    "    augmented_image_path = os.path.join(train_images_folder, aug_image_id)\n",
    "    cv2.imwrite(augmented_image_path, image_aug)\n",
    "\n",
    "    # Save augmented annotations in YOLO format\n",
    "    aug_label_path = os.path.join(train_labels_folder, f\"{os.path.splitext(aug_image_id)[0]}.txt\")\n",
    "    with open(aug_label_path, \"w\") as f:\n",
    "        for x_min, y_min, w, h in augmented_bboxes:\n",
    "            x_center = (x_min + w / 2) / img_width\n",
    "            y_center = (y_min + h / 2) / img_height\n",
    "            width = w / img_width\n",
    "            height = h / img_height\n",
    "            f.write(f\"1 {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\\n\")\n",
    "\n",
    "print(f\"\\nTotal images processed: {len(volume_bboxes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to original and augmented data\n",
    "original_images_folder = './yolo_data/train/images'\n",
    "original_labels_folder = './yolo_data/train/labels'\n",
    "augmented_images_folder = './augmented_images'\n",
    "augmented_labels_folder = './augmented_labels'\n",
    "\n",
    "# Merge augmented images into the training images folder\n",
    "for file in os.listdir(augmented_images_folder):\n",
    "    shutil.copy(os.path.join(augmented_images_folder, file), original_images_folder)\n",
    "\n",
    "# Merge augmented labels into the training labels folder\n",
    "for file in os.listdir(augmented_labels_folder):\n",
    "    shutil.copy(os.path.join(augmented_labels_folder, file), original_labels_folder)\n",
    "\n",
    "print(\"Augmented data merged successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_files = set(os.listdir('./yolo_data/train/images'))\n",
    "label_files = set(os.listdir('./yolo_data/train/labels'))\n",
    "\n",
    "# Strip extensions for comparison\n",
    "image_ids = {os.path.splitext(f)[0] for f in image_files}\n",
    "label_ids = {os.path.splitext(f)[0] for f in label_files}\n",
    "\n",
    "missing_labels = image_ids - label_ids\n",
    "if missing_labels:\n",
    "    print(f\"Images without labels: {missing_labels}\")\n",
    "else:\n",
    "    print(\"All images have corresponding labels!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Path to the folder containing YOLO labels\n",
    "train_labels_folder = './yolo_data/train/labels'\n",
    "\n",
    "# Initialize counters\n",
    "class_counts = {\"holds\": 0, \"volumes\": 0}\n",
    "\n",
    "# Iterate over label files\n",
    "for label_file in os.listdir(train_labels_folder):\n",
    "    label_path = os.path.join(train_labels_folder, label_file)\n",
    "    with open(label_path, 'r') as f:\n",
    "        for line in f:\n",
    "            class_id = int(line.split()[0])  # Class ID is the first value in each line\n",
    "            if class_id == 0:\n",
    "                class_counts[\"holds\"] += 1\n",
    "            elif class_id == 1:\n",
    "                class_counts[\"volumes\"] += 1\n",
    "\n",
    "# Print results\n",
    "print(f\"Total annotations for 'holds': {class_counts['holds']}\")\n",
    "print(f\"Total annotations for 'volumes': {class_counts['volumes']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset\n",
    "- Reads image files and their corresponding label files in YOLO format.\n",
    "- Resizes images to a consistent size (`640x640`).\n",
    "\n",
    "### Oversample Minority Class\n",
    "- Identifies images containing a specific class (e.g., \"volumes\") using the class ID.\n",
    "- Duplicates these images in the dataset (`oversample_factor`) to balance class frequencies.\n",
    "\n",
    "### Prepare Data for Training\n",
    "- Converts images to tensors and normalizes pixel values.\n",
    "- Converts bounding box labels into tensors for compatibility with PyTorch.\n",
    "\n",
    "### Handle Variable-Sized Labels\n",
    "- Implements a custom `collate_fn` to handle batches of data with varying numbers of bounding boxes per image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "class YoloDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir, class_to_oversample=1, oversample_factor=2, transform=None, img_size=(640, 640)):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.transform = transform\n",
    "        self.img_size = img_size  # Target size for resizing\n",
    "        self.class_to_oversample = class_to_oversample\n",
    "        self.oversample_factor = oversample_factor\n",
    "\n",
    "        # Load all images and labels\n",
    "        self.images = os.listdir(image_dir)\n",
    "        self.labels = os.listdir(label_dir)\n",
    "\n",
    "        # Map images to their labels\n",
    "        self.data = [\n",
    "            (os.path.join(image_dir, img), os.path.join(label_dir, f\"{os.path.splitext(img)[0]}.txt\"))\n",
    "            for img in self.images\n",
    "            if os.path.exists(os.path.join(label_dir, f\"{os.path.splitext(img)[0]}.txt\"))\n",
    "        ]\n",
    "\n",
    "        # Identify images containing the class to oversample\n",
    "        self.oversample_data = []\n",
    "        for img_path, label_path in self.data:\n",
    "            with open(label_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    class_id = int(line.split()[0])  # Class ID is the first value in each line\n",
    "                    if class_id == self.class_to_oversample:\n",
    "                        self.oversample_data.append((img_path, label_path))\n",
    "                        break\n",
    "\n",
    "        # Combine original data with oversampled data\n",
    "        self.data += self.oversample_data * (self.oversample_factor - 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label_path = self.data[idx]\n",
    "        image = cv2.imread(img_path)\n",
    "        if image is None:\n",
    "            raise ValueError(f\"Could not read image: {img_path}\")\n",
    "\n",
    "        # Resize or pad image to fixed size\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = cv2.resize(image, self.img_size)  # Resize image to fixed size\n",
    "\n",
    "        with open(label_path, 'r') as f:\n",
    "            labels = []\n",
    "            for line in f:\n",
    "                class_id, x_center, y_center, width, height = map(float, line.strip().split())\n",
    "                # Adjust label coordinates if resizing\n",
    "                labels.append([class_id, x_center, y_center, width, height])\n",
    "\n",
    "        labels = torch.tensor(labels)\n",
    "\n",
    "        # Apply transformations if specified\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Convert image to tensor and normalize\n",
    "        image = torch.tensor(image).permute(2, 0, 1).float() / 255.0\n",
    "\n",
    "        return image, labels\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to handle variable-sized labels (bounding boxes).\n",
    "    \"\"\"\n",
    "    images = torch.stack([item[0] for item in batch])  # Stack images into a tensor\n",
    "    labels = [item[1] for item in batch]  # Keep labels as a list of tensors\n",
    "    return images, labels\n",
    "\n",
    "\n",
    "# Initialize Dataset and DataLoader\n",
    "if __name__ == \"__main__\":\n",
    "    # Paths to training data\n",
    "    train_image_dir = './yolo_data/train/images'\n",
    "    train_label_dir = './yolo_data/train/labels'\n",
    "\n",
    "    dataset = YoloDataset(\n",
    "        image_dir=train_image_dir,\n",
    "        label_dir=train_label_dir,\n",
    "        class_to_oversample=1,  # Oversample 'volumes' class\n",
    "        oversample_factor=2,   # Oversample each 'volumes' image twice\n",
    "        img_size=(640, 640)    # Target size for resizing\n",
    "    )\n",
    "    dataloader = DataLoader(dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "    # Iterate through DataLoader\n",
    "    for images, labels in dataloader:\n",
    "        print(f\"Batch size: {len(images)}\")\n",
    "        print(f\"First image shape: {images[0].shape}\")\n",
    "        print(f\"Labels for first batch item: {labels[0]}\")\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Paths to training data\n",
    "    train_image_dir = './yolo_data/train/images'\n",
    "    train_label_dir = './yolo_data/train/labels'\n",
    "\n",
    "    # Initialize Dataset\n",
    "    dataset = YoloDataset(\n",
    "        image_dir=train_image_dir,\n",
    "        label_dir=train_label_dir,\n",
    "        class_to_oversample=1,  # Oversample 'volumes' class\n",
    "        oversample_factor=2,   # Oversample each 'volumes' image twice\n",
    "        img_size=(640, 640)    # Target size for resizing\n",
    "    )\n",
    "\n",
    "    # Initialize DataLoader with custom collate_fn\n",
    "    dataloader = DataLoader(\n",
    "        dataset, \n",
    "        batch_size=16, \n",
    "        shuffle=True, \n",
    "        collate_fn=collate_fn  # Use the custom collate function\n",
    "    )\n",
    "\n",
    "    # Iterate through DataLoader\n",
    "    for images, labels in dataloader:\n",
    "        print(f\"Batch size: {len(images)}\")\n",
    "        print(f\"First image shape: {images[0].shape}\")\n",
    "        print(f\"Labels for first batch item: {labels[0]}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_sample(image, labels, class_names=[\"hold\", \"volume\"]):\n",
    "    \"\"\"\n",
    "    Visualize an image with bounding boxes.\n",
    "    Args:\n",
    "        image (Tensor): Image tensor of shape [3, H, W].\n",
    "        labels (Tensor): Bounding box tensor of shape [N, 5].\n",
    "        class_names (list): List of class names.\n",
    "    \"\"\"\n",
    "    image = image.permute(1, 2, 0).numpy()  # Convert to HWC format\n",
    "    plt.imshow(image)\n",
    "\n",
    "    for label in labels:\n",
    "        class_id, x_center, y_center, width, height = label\n",
    "        x_min = (x_center - width / 2) * image.shape[1]\n",
    "        y_min = (y_center - height / 2) * image.shape[0]\n",
    "        x_max = (x_center + width / 2) * image.shape[1]\n",
    "        y_max = (y_center + height / 2) * image.shape[0]\n",
    "\n",
    "        # Draw bounding box\n",
    "        rect = plt.Rectangle(\n",
    "            (x_min, y_min), x_max - x_min, y_max - y_min, \n",
    "            edgecolor=\"red\", facecolor=\"none\", linewidth=2\n",
    "        )\n",
    "        plt.gca().add_patch(rect)\n",
    "        plt.text(\n",
    "            x_min, y_min - 5, class_names[int(class_id)],\n",
    "            color=\"red\", fontsize=10, bbox=dict(facecolor=\"white\", alpha=0.5)\n",
    "        )\n",
    "    plt.show()\n",
    "\n",
    "# Visualize a batch\n",
    "for images, labels in dataloader:\n",
    "    for i in range(3):  # Visualize the first 3 images in the batch\n",
    "        visualize_sample(images[i], labels[i])\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Distribution After Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def check_class_distribution(dataset):\n",
    "    \"\"\"\n",
    "    Check the distribution of classes (e.g., volumes vs. holds) in the dataset.\n",
    "    Args:\n",
    "        dataset: YoloDataset instance.\n",
    "    Returns:\n",
    "        dict: Count of instances for each class.\n",
    "    \"\"\"\n",
    "    class_counts = Counter()\n",
    "\n",
    "    for _, labels in dataset:\n",
    "        for label in labels:\n",
    "            class_id = int(label[0])  # First element in label is the class ID\n",
    "            class_counts[class_id] += 1\n",
    "\n",
    "    return class_counts\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Paths to training data\n",
    "    train_image_dir = './yolo_data/train/images'\n",
    "    train_label_dir = './yolo_data/train/labels'\n",
    "\n",
    "    # Initialize the dataset\n",
    "    dataset = YoloDataset(\n",
    "        image_dir=train_image_dir,\n",
    "        label_dir=train_label_dir,\n",
    "        class_to_oversample=1,  # Oversample 'volumes' class\n",
    "        oversample_factor=2,   # Oversample each 'volumes' image twice\n",
    "        img_size=(640, 640)    # Target size for resizing\n",
    "    )\n",
    "\n",
    "    # Check class distribution\n",
    "    distribution = check_class_distribution(dataset)\n",
    "    print(f\"Class distribution after oversampling: {distribution}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Configuration\n",
    "- Adjust Class Weights\n",
    "    - Since holds (class 0) are significantly more frequent than volumes (class 1), we adjust the class weights in the loss function to prioritize learning the minority class (volumes).\n",
    "-  hyp.yaml\n",
    "    - cls: 1.5   # Increased for minority class 'volumes'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run YOLO in yolov5 folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Code for running YOLO  \n",
    "python train.py --img 640 --batch 16 --epochs 50 --data yolo_data/data.yaml --weights yolov5s.pt --project runs --name hold_volume_detection\n",
    "\n",
    "- current:   \n",
    "python train.py --img 416 --batch 8 --epochs 10 --data yolo_data/data.yaml --weights yolov5n.pt --project runs --name hold_volume_detection --workers 4\n",
    "Results saved to runs\\hold_volume_detection5\n",
    "\n",
    "python train.py --img 416 --batch 8 --epochs 30 --data ./yolo_data/data.yaml --cfg ./models/yolov5s.yaml --weights yolov5s.pt --hyp ./runs/hold_volume_detection8/hyp.yaml --workers 4\n",
    "\n",
    "- to resume a run  \n",
    "python train.py --resume\n",
    "\n",
    "- can try this  \n",
    "python train.py --img 416 --batch 8 --epochs 10 --data yolo_data/data.yaml --weights yolov5n.pt --hyp hyp.scratch-low.yaml --workers 4\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the current working directory\n",
    "script_dir = os.getcwd()\n",
    "\n",
    "\n",
    "# Change directory to the YOLOv5 folder\n",
    "yolo_folder = os.path.join(script_dir, \"yolov5\")\n",
    "os.chdir(yolo_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Verify the working directory\n",
    "print(f\"Changed working directory to: {os.getcwd()}\")\n",
    "\n",
    "# Add YOLOv5 folder to Python path\n",
    "yolov5_path = os.path.join(script_dir, \"yolov5\")\n",
    "sys.path.append(yolov5_path)\n",
    "\n",
    "# Import YOLOv5 components\n",
    "from models.common import DetectMultiBackend\n",
    "\n",
    "# Set paths\n",
    "model_path = os.path.join(yolov5_path, \"runs/hold_volume_detection8/weights/best.pt\")\n",
    "input_folder = os.path.join(yolov5_path, \"data/test_image\")\n",
    "output_folder = os.path.join(yolov5_path, \"data/test_output\")\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the YOLOv5 model\n",
    "model = DetectMultiBackend(model_path, device=torch.device('cpu'))\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize statistics\n",
    "stats = {\"class\": [], \"confidence\": [], \"image\": [], \"x1\": [], \"y1\": [], \"x2\": [], \"y2\": [], \"cls\": []}\n",
    "\n",
    "# Loop through all images in the input folder\n",
    "image_files = [f for f in os.listdir(input_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "for image_name in tqdm(image_files, desc=\"Processing images\", unit=\"image\"):\n",
    "    try:\n",
    "        image_path = os.path.join(input_folder, image_name)\n",
    "\n",
    "        # Load image\n",
    "        img = cv2.imread(image_path)\n",
    "        if img is None:\n",
    "            print(f\"Warning: Unable to read image {image_name}. Skipping.\")\n",
    "            continue\n",
    "        img_height, img_width = img.shape[:2]\n",
    "\n",
    "        # Preprocess image for YOLO\n",
    "        img_resized = cv2.resize(img, (416, 416))  # Resize to YOLO input size\n",
    "        img_tensor = torch.from_numpy(img_resized).permute(2, 0, 1).float().div(255.0).unsqueeze(0)  # Normalize and add batch dimension\n",
    "\n",
    "        # Run inference\n",
    "        results = model(img_tensor)\n",
    "\n",
    "        # Process predictions\n",
    "        for det in results[0]:  # Get the first level of predictions\n",
    "            # Ensure det is a tensor and process it\n",
    "            if isinstance(det, torch.Tensor):\n",
    "                det = det.cpu().numpy()  # Convert tensor to numpy array\n",
    "\n",
    "            for pred in det:  # Iterate through individual predictions\n",
    "                # Extract relevant values: bbox, confidence, and class\n",
    "                if pred.shape[0] >= 6:  # Ensure there are enough elements\n",
    "                    x_center, y_center, width, height = pred[:4]\n",
    "                    conf = pred[4]  # Fifth value is confidence\n",
    "                    cls = pred[5]  # Sixth value is class index\n",
    "\n",
    "                    \n",
    "\n",
    "                    # Confidence threshold\n",
    "                    if conf < 0.5:  # Ignore predictions with low confidence\n",
    "                        print(f\"Skipped prediction with low confidence: {conf}\")\n",
    "                        continue\n",
    "\n",
    "                    # Convert from center-size format to corner coordinates\n",
    "                    x1 = int((x_center - width / 2) / 416 * img_width)\n",
    "                    y1 = int((y_center - height / 2) / 416 * img_height)\n",
    "                    x2 = int((x_center + width / 2) / 416 * img_width)\n",
    "                    y2 = int((y_center + height / 2) / 416 * img_height)\n",
    "\n",
    "                    class_id = 0 if cls <= 0.5 else 1  # 0 for holds, 1 for volumes\n",
    "\n",
    "                    # Save bounding box locations\n",
    "                    stats[\"class\"].append(int(class_id))\n",
    "                    stats[\"confidence\"].append(float(conf))\n",
    "                    stats[\"image\"].append(image_name)\n",
    "                    stats[\"x1\"].append(x1)\n",
    "                    stats[\"y1\"].append(y1)\n",
    "                    stats[\"x2\"].append(x2)\n",
    "                    stats[\"y2\"].append(y2)\n",
    "                    stats[\"cls\"].append(cls)\n",
    "\n",
    "                    label = f\"{'Hold' if cls <= 0.5 else 'Volume'} {conf:.2f}\"\n",
    "                    color = (255, 0, 0) if label=='Hold' else (0, 0, 255)  # Blue for holds, red for volumes\n",
    "                    cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)\n",
    "                    # Draw bounding boxes\n",
    "                    cv2.putText(img, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "        # Save the annotated image\n",
    "        output_path = os.path.join(output_folder, image_name)\n",
    "        cv2.imwrite(output_path, img)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_name}: {e}\")\n",
    "\n",
    "# Save statistics to a CSV file\n",
    "stats_df = pd.DataFrame(stats)\n",
    "stats_df.to_csv(os.path.join(output_folder, \"test_statistics.csv\"), index=False)\n",
    "\n",
    "print(f\"Processed {len(image_files)} images. Results saved to {output_folder}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou(box1, box2):\n",
    "    \"\"\"\n",
    "    Calculate Intersection over Union (IoU) between two bounding boxes.\n",
    "    box1, box2: (x1, y1, x2, y2)\n",
    "    \"\"\"\n",
    "    x1_inter = max(box1[0], box2[0])\n",
    "    y1_inter = max(box1[1], box2[1])\n",
    "    x2_inter = min(box1[2], box2[2])\n",
    "    y2_inter = min(box1[3], box2[3])\n",
    "\n",
    "    inter_area = max(0, x2_inter - x1_inter + 1) * max(0, y2_inter - y1_inter + 1)\n",
    "    box1_area = (box1[2] - box1[0] + 1) * (box1[3] - box1[1] + 1)\n",
    "    box2_area = (box2[2] - box2[0] + 1) * (box2[3] - box2[1] + 1)\n",
    "    union_area = box1_area + box2_area - inter_area\n",
    "\n",
    "    return inter_area / union_area if union_area != 0 else 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_maximum_suppression(df, iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Apply Non-Maximum Suppression (NMS) to the bounding boxes in a DataFrame.\n",
    "    df: DataFrame containing 'x1', 'y1', 'x2', 'y2', 'confidence', 'class', 'image'\n",
    "    iou_threshold: IoU threshold to suppress overlapping boxes\n",
    "    \"\"\"\n",
    "    nms_results = []\n",
    "\n",
    "    # Process each image separately\n",
    "    for image, group in df.groupby('image'):\n",
    "        boxes = group[['x1', 'y1', 'x2', 'y2']].values\n",
    "        confidences = group['confidence'].values\n",
    "        classes = group['class'].values\n",
    "\n",
    "        # Sort boxes by confidence score in descending order\n",
    "        indices = confidences.argsort()[::-1]\n",
    "        boxes = boxes[indices]\n",
    "        confidences = confidences[indices]\n",
    "        classes = classes[indices]\n",
    "        group = group.iloc[indices]  # Sort the group DataFrame accordingly\n",
    "\n",
    "        suppressed = set()\n",
    "        for i in range(len(boxes)):\n",
    "            if i in suppressed:\n",
    "                continue\n",
    "            current_box = boxes[i]\n",
    "            nms_results.append(group.iloc[i])  # Keep the box with the highest confidence\n",
    "            for j in range(i + 1, len(boxes)):\n",
    "                if j in suppressed:\n",
    "                    continue\n",
    "                iou_score = iou(current_box, boxes[j])\n",
    "                # Suppress boxes based on IoU only, ignoring class differences\n",
    "                if iou_score > iou_threshold:\n",
    "                    suppressed.add(j)\n",
    "\n",
    "    # Convert the results back to a DataFrame\n",
    "    return pd.DataFrame(nms_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the statistics DataFrame\n",
    "stats_path = os.path.join(output_folder, \"test_statistics.csv\")\n",
    "stats_df = pd.read_csv(stats_path)\n",
    "\n",
    "# Apply NMS\n",
    "nms_df = non_maximum_suppression(stats_df, iou_threshold=0.5)\n",
    "\n",
    "# Save the NMS results to a new CSV file\n",
    "nms_path = os.path.join(output_folder, \"nms_statistics.csv\")\n",
    "nms_df.to_csv(nms_path, index=False)\n",
    "\n",
    "print(f\"NMS applied. Results saved to {nms_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def improved_non_maximum_suppression(df, iou_threshold=0.5, sigma=0.5):\n",
    "    \"\"\"\n",
    "    Apply Non-Maximum Suppression (NMS) with improved confidence calculation.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame containing 'x1', 'y1', 'x2', 'y2', 'confidence', 'class', 'image'.\n",
    "        iou_threshold: IoU threshold for suppressing overlapping boxes.\n",
    "        sigma: Decay rate for soft confidence adjustment.\n",
    "\n",
    "    Returns:\n",
    "        A DataFrame containing the filtered bounding boxes with improved confidence logic.\n",
    "    \"\"\"\n",
    "    nms_results = []\n",
    "\n",
    "    # Process each image separately\n",
    "    for image, group in df.groupby('image'):\n",
    "        boxes = group[['x1', 'y1', 'x2', 'y2']].values\n",
    "        confidences = group['confidence'].values\n",
    "        classes = group['class'].values\n",
    "\n",
    "        # Sort boxes by confidence score in descending order\n",
    "        indices = confidences.argsort()[::-1]\n",
    "        boxes = boxes[indices]\n",
    "        confidences = confidences[indices]\n",
    "        classes = classes[indices]\n",
    "        group = group.iloc[indices]  # Sort the group DataFrame accordingly\n",
    "\n",
    "        suppressed = set()\n",
    "        for i in range(len(boxes)):\n",
    "            if i in suppressed:\n",
    "                continue\n",
    "            current_box = boxes[i]\n",
    "            current_confidence = confidences[i]\n",
    "            weighted_confidence = current_confidence  # Initialize with original confidence\n",
    "\n",
    "            # Keep the box with the highest confidence\n",
    "            nms_results.append(group.iloc[i])\n",
    "\n",
    "            for j in range(i + 1, len(boxes)):\n",
    "                if j in suppressed:\n",
    "                    continue\n",
    "\n",
    "                iou_score = iou(current_box, boxes[j])\n",
    "                if iou_score > iou_threshold:\n",
    "                    # Adjust confidence based on overlap using a soft suppression formula\n",
    "                    confidences[j] *= np.exp(-iou_score**2 / sigma) + 0.1\n",
    "                    \n",
    "                    # Update the weighted confidence for the current box\n",
    "                    weighted_confidence += confidences[j] * np.exp(-iou_score**2 / sigma)\n",
    "                    \n",
    "                    suppressed.add(j)\n",
    "\n",
    "            # Update confidence of the selected box in the results\n",
    "            nms_results[-1]['confidence'] = weighted_confidence\n",
    "\n",
    "    # Convert the results back to a DataFrame\n",
    "    return pd.DataFrame(nms_results)\n",
    "\n",
    "\n",
    "# Apply the improved NMS\n",
    "nms_df_improved = improved_non_maximum_suppression(stats_df, iou_threshold=0.5)\n",
    "\n",
    "# Save the results to a new CSV file\n",
    "nms_path = os.path.join(output_folder, \"nms_statistics_improved.csv\")\n",
    "nms_df_improved.to_csv(nms_path, index=False)\n",
    "\n",
    "print(f\"Improved NMS applied. Results saved to {nms_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def improved_non_maximum_suppression(df, iou_threshold=0.4, sigma=0.4):\n",
    "    \"\"\"\n",
    "    Apply Non-Maximum Suppression (NMS) with improved confidence and class recalculation.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame containing 'x1', 'y1', 'x2', 'y2', 'confidence', 'class', 'image'.\n",
    "        iou_threshold: IoU threshold for suppressing overlapping boxes.\n",
    "        sigma: Decay rate for soft confidence adjustment.\n",
    "\n",
    "    Returns:\n",
    "        A DataFrame containing the filtered bounding boxes with recalculated confidence and class.\n",
    "    \"\"\"\n",
    "    nms_results = []\n",
    "\n",
    "    # Process each image separately\n",
    "    for image, group in df.groupby('image'):\n",
    "        boxes = group[['x1', 'y1', 'x2', 'y2']].values\n",
    "        confidences = group['confidence'].values\n",
    "        classes = group['class'].values\n",
    "\n",
    "        # Sort boxes by confidence score in descending order\n",
    "        indices = confidences.argsort()[::-1]\n",
    "        boxes = boxes[indices]\n",
    "        confidences = confidences[indices]\n",
    "        classes = classes[indices]\n",
    "        group = group.iloc[indices]  # Sort the group DataFrame accordingly\n",
    "\n",
    "        suppressed = set()\n",
    "        for i in range(len(boxes)):\n",
    "            if i in suppressed:\n",
    "                continue\n",
    "            current_box = boxes[i]\n",
    "            current_confidence = confidences[i]\n",
    "            weighted_confidence = current_confidence  # Initialize with original confidence\n",
    "            weighted_class_sum = current_confidence * classes[i]  # Initialize weighted class sum\n",
    "            weight_sum = current_confidence  # Initialize sum of weights\n",
    "\n",
    "            # Keep the box with the highest confidence\n",
    "            nms_results.append(group.iloc[i])\n",
    "\n",
    "            for j in range(i + 1, len(boxes)):\n",
    "                if j in suppressed:\n",
    "                    continue\n",
    "\n",
    "                iou_score = iou(current_box, boxes[j])\n",
    "                if iou_score > iou_threshold:\n",
    "                    # Adjust confidence based on overlap using a soft suppression formula\n",
    "                    confidence_adjustment = np.exp(-iou_score**2 / sigma)\n",
    "                    confidences[j] *= confidence_adjustment\n",
    "                    \n",
    "                    # Update weighted sums for confidence and class\n",
    "                    weight_sum += confidences[j]\n",
    "                    weighted_class_sum += confidences[j] * classes[j]\n",
    "                    \n",
    "                    suppressed.add(j)\n",
    "\n",
    "            # Finalize the recalculated confidence and class\n",
    "            recalculated_confidence = weighted_confidence\n",
    "            recalculated_class = int(round(weighted_class_sum / weight_sum))\n",
    "\n",
    "            # Update confidence and class of the selected box in the results\n",
    "            nms_results[-1]['confidence'] = recalculated_confidence\n",
    "            nms_results[-1]['class'] = recalculated_class\n",
    "\n",
    "    # Convert the results back to a DataFrame\n",
    "    return pd.DataFrame(nms_results)\n",
    "\n",
    "\n",
    "# Apply the improved NMS with class recalculation\n",
    "nms_df_recalculated = improved_non_maximum_suppression(stats_df, iou_threshold=0.5)\n",
    "\n",
    "# Save the results to a new CSV file\n",
    "nms_path = os.path.join(output_folder, \"nms_statistics_recalculated.csv\")\n",
    "nms_df_recalculated.to_csv(nms_path, index=False)\n",
    "\n",
    "print(f\"Improved NMS with class recalculation applied. Results saved to {nms_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nms_df = nms_df.reset_index(drop=True)\n",
    "nms_df_recalculated = nms_df_recalculated.reset_index(drop=True)\n",
    "\n",
    "nms_df_recalculated = nms_df_recalculated.rename(columns=lambda x: f\"{x}_weighted\")\n",
    "merged_nms = pd.concat([nms_df, nms_df_recalculated], axis=1)\n",
    "merged_nms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter mismatched rows\n",
    "mismatched_df = merged_nms[merged_nms['class'] != merged_nms['class_weighted']]\n",
    "\n",
    "# Select relevant columns\n",
    "mismatched_summary = mismatched_df[[\n",
    "    'class', 'image', 'confidence','x1', 'y1', 'x2', 'y2', 'class_weighted', 'confidence_weighted'\n",
    "]]\n",
    "\n",
    "output_csv_path = \"mismatched_summary.csv\"\n",
    "mismatched_summary.to_csv(output_csv_path, index=False)\n",
    "\n",
    "mismatched_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output folder for mismatched NMS results\n",
    "mismatched_nms_output_folder = os.path.join(output_folder, \"mismatched_nms_output\")\n",
    "os.makedirs(mismatched_nms_output_folder, exist_ok=True)\n",
    "\n",
    "# Loop through each image in the mismatched summary and draw bounding boxes\n",
    "for image_name, group in mismatched_summary.groupby('image'):\n",
    "    image_path = os.path.join(input_folder, image_name)\n",
    "    output_path = os.path.join(mismatched_nms_output_folder, image_name)\n",
    "\n",
    "    # Load image\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        print(f\"Warning: Unable to read image {image_name}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Draw bounding boxes for mismatched predictions\n",
    "    for _, row in group.iterrows():\n",
    "        x1, y1, x2, y2 = int(row['x1']), int(row['y1']), int(row['x2']), int(row['y2'])\n",
    "        cls = row['class']\n",
    "        cls_weighted = row['class_weighted']\n",
    "        conf = row['confidence']\n",
    "        conf_weighted = row['confidence_weighted']\n",
    "\n",
    "        # Define color: blue for standard, red for weighted\n",
    "        color_standard = (255, 0, 0)  # Blue\n",
    "        color_weighted = (0, 0, 255)  # Red\n",
    "\n",
    "        # Draw bounding box for the standard prediction\n",
    "        label_standard = f\"Std: {'Hold' if cls == 0 else 'Volume'} {conf:.2f}\"\n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), color_standard, 2)\n",
    "        cv2.putText(img, label_standard, (x1, y1 - 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color_standard, 2)\n",
    "\n",
    "        # Draw bounding box for the weighted prediction\n",
    "        label_weighted = f\"Wtd: {'Hold' if cls_weighted == 0 else 'Volume'} {conf_weighted:.2f}\"\n",
    "        cv2.putText(img, label_weighted, (x1, y2 + 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color_weighted, 2)\n",
    "\n",
    "    # Save the annotated image\n",
    "    cv2.imwrite(output_path, img)\n",
    "\n",
    "print(f\"Mismatched NMS drawings saved to: {mismatched_nms_output_folder}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output folder for NMS results\n",
    "nms_output_folder = os.path.join(output_folder, \"nms_output\")\n",
    "os.makedirs(nms_output_folder, exist_ok=True)\n",
    "\n",
    "# Load NMS results\n",
    "nms_df = pd.read_csv(os.path.join(output_folder, \"nms_statistics.csv\"))\n",
    "\n",
    "# Loop through each image and draw bounding boxes\n",
    "for image_name, group in nms_df.groupby('image'):\n",
    "    image_path = os.path.join(input_folder, image_name)\n",
    "    output_path = os.path.join(nms_output_folder, image_name)\n",
    "\n",
    "    # Load image\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        print(f\"Warning: Unable to read image {image_name}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Draw bounding boxes from NMS\n",
    "    for _, row in group.iterrows():\n",
    "        x1, y1, x2, y2 = int(row['x1']), int(row['y1']), int(row['x2']), int(row['y2'])\n",
    "        cls = float(row['cls'])\n",
    "        conf = float(row['confidence'])\n",
    "\n",
    "        # Define color: blue for holds (class 0), red for volumes (class 1)\n",
    "        color = (255, 0, 0) if cls >= 0.5 else (0, 0, 255)\n",
    "        label = f\"{'Hold' if cls >= 0.5 else 'Volume'} {conf:.2f}\"\n",
    "\n",
    "        # Draw rectangle and label\n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)\n",
    "        cv2.putText(img, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "    # Save the annotated image\n",
    "    cv2.imwrite(output_path, img)\n",
    "\n",
    "print(f\"NMS drawings saved to: {nms_output_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('../test_coco_annotations.csv')\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output folder for NMS results\n",
    "nms_output_folder = os.path.join(output_folder, \"nms_output\")\n",
    "os.makedirs(nms_output_folder, exist_ok=True)\n",
    "\n",
    "# Load NMS results\n",
    "nms_df = pd.read_csv(os.path.join(output_folder, \"nms_statistics.csv\"))\n",
    "\n",
    "# Loop through each image and draw bounding boxes\n",
    "for image_name, group in nms_df.groupby('image'):\n",
    "    image_path = os.path.join(input_folder, image_name)\n",
    "    output_path = os.path.join(nms_output_folder, image_name)\n",
    "\n",
    "    # Load image\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        print(f\"Warning: Unable to read image {image_name}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Draw bounding boxes from NMS\n",
    "    for _, row in group.iterrows():\n",
    "        x1, y1, x2, y2 = int(row['x1']), int(row['y1']), int(row['x2']), int(row['y2'])\n",
    "        cls = float(row['cls'])\n",
    "        conf = float(row['confidence'])\n",
    "\n",
    "        # Define color: blue for holds (class 0), red for volumes (class 1)\n",
    "        color = (255, 0, 0) if cls >= 0.5 else (0, 0, 255)\n",
    "        label = f\"{'Hold' if cls >= 0.5 else 'Volume'} {conf:.2f}\"\n",
    "\n",
    "        # Draw rectangle and label\n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)\n",
    "        cv2.putText(img, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "    # Save the annotated image\n",
    "    cv2.imwrite(output_path, img)\n",
    "\n",
    "print(f\"NMS drawings saved to: {nms_output_folder}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "model_path = './runs/hold_volume_detection8/weights/best.pt'  # Trained YOLO model\n",
    "validation_images_path = './yolo_data/valid/images'  # Validation images folder\n",
    "nms_output_path = './data/valid_output/nms_statistics_eval.csv'  # NMS results output\n",
    "\n",
    "# Ensure output folder exists\n",
    "os.makedirs(os.path.dirname(nms_output_path), exist_ok=True)\n",
    "\n",
    "# Function to predict validation images and save NMS results\n",
    "def predict_validation_images(model_path, validation_images_path, nms_output_path):\n",
    "    # Load YOLO model\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = DetectMultiBackend(model_path, device=device)\n",
    "\n",
    "    stats = {\"image_name\": [], \"predicted_label\": [], \"confidence\": [], \"x1\": [], \"y1\": [], \"x2\": [], \"y2\": []}\n",
    "    image_files = [f for f in os.listdir(validation_images_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "    for image_name in tqdm(image_files, desc=\"Processing validation images\"):\n",
    "        image_path = os.path.join(validation_images_path, image_name)\n",
    "        img = cv2.imread(image_path)\n",
    "\n",
    "        if img is None:\n",
    "            print(f\"Warning: Unable to read image {image_name}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        img_height, img_width = img.shape[:2]\n",
    "        img_resized = cv2.resize(img, (416, 416))\n",
    "        img_tensor = torch.from_numpy(img_resized).permute(2, 0, 1).float().div(255.0).unsqueeze(0)\n",
    "\n",
    "        # Run inference\n",
    "        results = model(img_tensor)\n",
    "\n",
    "        # Process predictions\n",
    "        for det in results[0]:\n",
    "            if isinstance(det, torch.Tensor):\n",
    "                det = det.cpu().numpy()\n",
    "\n",
    "            for pred in det:\n",
    "                if pred.shape[0] >= 6 and pred[4] > 0.5:  # Confidence threshold\n",
    "                    x_center, y_center, width, height = pred[:4]\n",
    "                    conf = pred[4]\n",
    "                    cls = pred[5]\n",
    "\n",
    "                    x1 = int((x_center - width / 2) * img_width)\n",
    "                    y1 = int((y_center - height / 2) * img_height)\n",
    "                    x2 = int((x_center + width / 2) * img_width)\n",
    "                    y2 = int((y_center + height / 2) * img_height)\n",
    "\n",
    "                    stats[\"image_name\"].append(image_name)\n",
    "                    stats[\"predicted_label\"].append(int(cls))\n",
    "                    stats[\"confidence\"].append(float(conf))\n",
    "                    stats[\"x1\"].append(x1)\n",
    "                    stats[\"y1\"].append(y1)\n",
    "                    stats[\"x2\"].append(x2)\n",
    "                    stats[\"y2\"].append(y2)\n",
    "\n",
    "    # Save predictions to CSV\n",
    "    pd.DataFrame(stats).to_csv(nms_output_path, index=False)\n",
    "    print(f\"NMS results saved to: {nms_output_path}\")\n",
    "\n",
    "# Run validation\n",
    "predict_validation_images(model_path, validation_images_path, nms_output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "predictions_path = './yolov5/data/valid_output/nms_statistics_eval.csv'\n",
    "valid_annotations_path = '../valid_coco_annotations.csv'\n",
    "\n",
    "# Load CSVs\n",
    "predictions = pd.read_csv(predictions_path)\n",
    "ground_truth = pd.read_csv(valid_annotations_path)\n",
    "\n",
    "# Standardize image names in both datasets\n",
    "predictions['image_name'] = predictions['image_name'].apply(lambda x: os.path.basename(x))\n",
    "ground_truth['image_name'] = ground_truth['FileName'].apply(lambda x: os.path.basename(x))\n",
    "\n",
    "\n",
    "print(predictions.head())\n",
    "print(ground_truth.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge datasets on image_name\n",
    "merged = pd.merge(predictions, ground_truth, on='image_name', how='inner')\n",
    "\n",
    "# Check the merged data\n",
    "merged.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
